# -*- coding: utf-8 -*-
"""CAFCA-Colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11vna0fMKAOAsYnR20bYxKXjth0ifAMon

# **Colab Initialization**
"""

# from google.colab import drive
from os.path import join
# drive.mount('/gdrive')
# gdrive_root = '/gdrive/My Drive/'

"""# **Interaction Model Abstraction**

Abstract the observable environmental sensing data & communication traces from the logs

## Log Parser
"""
import copy
import random
import os

target_scenario = 'OP_SUCCESS_RATE'  # INPUT: OP_SUCCESS_RATE or COLLISION
LOG_PATH = 'C:/Users/Hyun/IdeaProjects/StarPlateS/SoS_Extension/logs_full'
V_PATH = 'C:/Users/Hyun/IdeaProjects/CAFCA'
print('In Log Folder : ', os.listdir(LOG_PATH))

"""## Interaction model generator"""
def IMGenerator():
  IM = [] # A set of all interaction models ======> IM = [im0, im1, im2, im3, ...]
  FIM = [] # A set of failed interaction models
  curnt_id = -1
  f = open(join(V_PATH, target_scenario+'_Verification_Results.csv')) # To check the Verification results
  v_results = f.readlines()
  f.close()
  im = [] # an interaction model ======> im = [file_id, P/F, Interaction, Env]
  for filename in os.listdir(LOG_PATH):
    strings = filename.split('_')
    if int(strings[0]) != curnt_id: # Change to the new failure scenario id
      if len(im) == 4:
        IM.append(copy.deepcopy(im))
        if im[1] == "FALSE":
          FIM.append(copy.deepcopy(im))
      im.clear()
      if curnt_id != -1: # Check the progress in console
        print("> Finished\n")
      print("===========" + filename.split('_')[0] + ": Start =======")
      curnt_id = int(strings[0])
      im.append(curnt_id) # Add the file id
      for line in v_results:
        if '\\'+str(curnt_id)+'_' in line.split(',')[0]:  # Add the P/F results of the log file
          p_f = line.split(',')[1]
          im.append(p_f[:len(p_f)-1])
          break
    if 'vehicleData' in strings[1]: # Extract env data
      env = [] # ======> Env = [state0, state1, state2, ...] ordered chronologically
      f = open(join(LOG_PATH,filename), 'r')
      lines = f.readlines()
      state = [] # A single state (i.e. item) in a log file => [time, veh1, veh1_lane, veh1_loc, veh2, veh2_lane, veh2_loc, ...]
      for i in range(16, len(lines)): # For each line in log file
        line = lines[i]
        line_items = line.split(' ')
        count = 0
        veh_id = -1
        veh_pos = -1
        for j in range(1, len(line_items)): # Extract the information by each line
           if line_items[j] != '':
             if count == 0:
               time = line_items[j]
               if len(state) == 0:
                 state.append(time)
             elif count == 1:
               veh_id = line_items[j]
               state.append(veh_id)
             elif count == 2:
                state.append(line_items[j])
             elif count == 3:
               veh_pos = line_items[j]
               state.append(veh_pos)
             count += 1
        if count == 0:
          env.append(copy.deepcopy(state))
          state = []
      f.close()
      im.append(copy.deepcopy(env))
    elif 'plnData' in strings[1]: # Extract interaction data
      interaction = [] # ======> interaction = [message0, message1, message2, ...] ordered chronologically
      f = open(join(LOG_PATH,filename), 'r')
      lines = f.readlines()
      F_type = -1
      split_count = -1
      veh_roles = {}
      for i in range(16, len(lines)): # For each line in log file
        message = []
        line = lines[i]
        line = ' '.join(line.split()) # Preprocessing
        line = line.replace(' -', '')
        line_items = line.split(' ')
        if 'MF_Leave' in line: # FollowerLeave type checking for veh role analysis
          split_count = 2
        elif 'EF_Leave' in line:
          split_count = 1
        if len(line_items) > 5: # For each message items => [time, command_sent, sender, sender_role, receiver, receiver_role]
          time = line_items[0]
          message.append(time)
          command_sent = line_items[4]
          message.append(command_sent)
          if command_sent == 'MERGE_REQ': # Role managmenet code by each role changing CS-level operations
            veh_roles[line_items[1]] = 'Leader'
            if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
              veh_roles[line_items[5]] = 'Leader'
          elif command_sent == 'SPLIT_REQ':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles[line_items[1]] = 'Leader'
          elif command_sent == 'LEAVE_REQ':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles[line_items[1]] = 'Left'
            if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
              veh_roles[line_items[5]] = 'Leader'
          elif command_sent == 'VOTE_LEADER':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles[line_items[1]] = 'Left'
          # elif command_sent == 'ELECTED_LEADER':
          #   if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
          #     veh_roles[line_items[1]] = 'Leader'
          elif command_sent == 'MERGE_DONE':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles.pop(line_items[1])
          elif command_sent == 'SPLIT_DONE':
            split_count -= 1
            if split_count != 0: # Just SPLIT operation && the first SPLIT in MF-LEAVE && LEADER-LEAVE
              if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
                veh_roles[line_items[5]] = 'Leader'
            elif split_count == 0: # The last SPLIT in MF-LEAVE / EF-LEAVE
              if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
                veh_roles[line_items[5]] = 'Left'
          sender = line_items[1]
          message.append(sender)
          if line_items[1] not in veh_roles.keys():
            message.append('Follower')
          else:
            message.append(veh_roles[line_items[1]])
          receiver = line_items[5]
          message.append(receiver)
          if line_items[5] not in veh_roles.keys():
            message.append('Follower')
          else:
            message.append(veh_roles[line_items[5]])
        if len(message) != 0:
          interaction.append(copy.deepcopy(message))
      f.close()
      im.append(copy.deepcopy(interaction))
  print(IM[random.randrange(0,int(len(os.listdir(LOG_PATH))/4))]) # Random print of a single m
  return IM, FIM

"""## Interaction model txt Writer

"""
def IMtoTxt(IM, fname):
  def ListToString(list_):
    ret = ''
    for sublist_ in list_:
      if ret != '':
        ret = ret[:-1]
        ret += '|'
      for item_ in sublist_:
        ret += str(item_) + ','
    return ret[:-1]

  # File write of IM
  f = open(join(V_PATH,fname), 'w')
  for im in IM:
    f.write(str(im[0]) + '/' + str(im[1]) + '/' + ListToString(im[2]) + '/' + ListToString(im[3]) + '\n')
  f.close()

"""## Interaction model txt reader

"""
def TxttoIM(fname):
  def StringToList(string_):
    ret = []
    sublists = string_.split('|')
    for sublist in sublists:
      sublist = sublist.split(',')
      for id, item in enumerate(sublist): # To remove newline in items
        if '\n' in item:
          sublist[id] = item.rstrip()
      ret.append(copy.deepcopy(sublist))
    return ret

  f = open(join(V_PATH, fname), 'r')
  IM_ = []
  lines = f.readlines()
  for line in lines:
    im_ = line.split('/')
    im_[0] = int(im_[0])
    im_[2] = StringToList(im_[2])
    im_[3] = StringToList(im_[3])
    IM_.append(copy.deepcopy(im_))
  print("FINISH TxttoIM function\n")
  return IM_

"""# **Fuzzy Clustering for Initial Pattern Mining**

Generate initial subsequential patterns by clustering based on the failed logs
Overlapping clustering approach is applied.

## Environmental state & message identity checker
"""

import numpy as np
from numpy import dot
from numpy.linalg import norm

# The function you need to call when you try to check the identity of messages with environmental states.
# Inputs: two messages, the two previously matched messages (None, if not exist), whole Env states for checking the specific points of Env states around the messages, delay_threshold, time_window size for Env state comparison, and env_similarity threshold
def ESMIC(msg_a, msg_b, msg_a_prev, msg_b_prev, env_a, env_b, d_threshold, time_window, env_sim_threshold):
  if not MCT(msg_a, msg_b) or not CalMessageDelay(msg_a, msg_b, msg_a_prev, msg_b_prev, d_threshold):
    return False
  env_sim = []
  for state_a in env_a:
    if float(state_a[0]) < float(msg_a[0]) - time_window or float(state_a[0]) > float(msg_a[0]) + time_window:
      continue
    for state_b in env_b:
      if float(state_b[0]) < float(msg_b[0]) - time_window or float(state_b[0]) > float(msg_b[0]) + time_window:
        continue
      print(state_a[0])
      print(state_b[0])
      env_sim.append(EnvStateCompare(state_a,state_b))
  env_sim = np.array(env_sim)
  print(env_sim)
  if np.max(env_sim) < env_sim_threshold: # np.mean -> np.max
    return False, np.max(env_sim)
  return True, np.max(env_sim)

def MCT(msg_a, msg_b): # Compare the identity of the two messages => [time, command_sent, sender, sender_role, receiver, receiver_role]
  return msg_a[1] == msg_b[1] and msg_a[3] == msg_b[3] and msg_a[5] == msg_b[5]

def CalMessageDelay(msg_a, msg_b, msg_a_prev, msg_b_prev, d_threshold): # Check the delivery intervals of the two messages
  if msg_a_prev == None or msg_b_prev == None:
    return True
  delay_a = float(msg_a[0]) - float(msg_a_prev[0])
  delay_b = float(msg_b[0]) - float(msg_b_prev[0])
  return abs(delay_a - delay_b) <= d_threshold

def EnvStateCompare(state_a, state_b): # Compare the identity of the two Env states => [time, veh1, veh1_lane, veh1_loc, veh2, veh2_lane, veh2_loc, ...]
  veh_a = EnvStatePreprocess(state_a)
  veh_b = EnvStatePreprocess(state_b)
  print(veh_a)
  print(veh_b)
  sim_values = []
  for distance_a in veh_a:
    base = np.array(distance_a)
    for distance_b in veh_b:
      if len(base) > len(distance_b):
        base = np.array(distance_b)
        for i in range(0,len(distance_a)-len(base)+1):
          sim_values.append(cos_sim(base, np.array(distance_a[i:i+len(base)])))
      else:
        for i in range(0,len(distance_b)-len(base)+1):
          sim_values.append(cos_sim(base, np.array(distance_b[i:i+len(base)])))
  print(sim_values)
  return max(sim_values)

def cos_sim(A, B):
  return dot(A, B)/(norm(A)*norm(B))

def EnvStatePreprocess(state): # Initially arrange the Env state data to dictionary & remove unrelated vehicle location data
  dic_state = {}
  ret = []
  for i in range(1,len(state)):
    if i % 3 == 1:
      veh_id = state[i]
    elif i % 3 == 2:
      lane = state[i]
    else:
      dic_state[veh_id] = (lane, state[i])
  ret.append(vehDistanceGeneration(dic_state,'veh'))
  ret.append(vehDistanceGeneration(dic_state,'veh1'))
  return ret

def vehDistanceGeneration(dic_state, id):
  veh_location = []
  for key in dic_state.keys():
    lane = dic_state[key][0]
    loc = dic_state[key][1]
    if lane == dic_state[id][0]:
      veh_location.append(float(loc))
    veh_location.sort(reverse=True)
  print(veh_location)
  veh_distance = []
  for i in range(0, len(veh_location)-1):
    veh_distance.append(Quantification(float(veh_location[i]) - float(veh_location[i+1])))
  return veh_distance

def Quantification(distance): # 2: Very far / 1: Far / 0 : Appropriate / -1 : Close / -2 : Very close
  inter_gap = 105
  intra_gap = 16.5
  if distance > 2.5 * inter_gap:
    return 2
  elif distance > 1.25 * inter_gap:
    return 1
  elif distance > intra_gap:
    return 0
  elif distance > 0.5 * intra_gap:
    return -1
  else:
    return -2

#print(ESMIC(IM_[1][2][18], IM_[1][2][18], None, None, IM_[1][3], IM_[1][3], 1.0, 5, 0.8))
# print(IM_[1][3][0])
# print(IM_[1][3][500])
# print(EnvStateCompare(IM_[1][3][0], IM_[1][3][500]))

"""###LCS Algorithm"""

# The function you need to call when you try to calculate the similarity based on the LCS and ESMIC function.
# Inputs: two models (pattern and input), d_threshold
# Outputs: The most critical LCS among possible LCS generation sets, the LCS similarity value with the pattern and input models.
def CAFCASimCal(im_pattern, im_input, d_threshold):
  p = 0.5
  q = 0.5
  generated_pattern, avg_env_sim = GetPattern(im_pattern, im_input, d_threshold)
  return p*(len(generated_pattern) / len(im_pattern) * len(im_input)) + q*avg_env_sim

def PatternExtractor(im_pattern, im_input, d_threshold):
  pattern = im_pattern[2]
  pattern_env = im_pattern[3]
  input = im_input[2]
  input_env = im_input[3]
  len_ptn = len(pattern)
  len_input = len(input)
  LCS = [[0]*(len_input+1) for i in range(len_ptn+1)]
  pattern_prev_msg = None
  input_prev_msg = None
  env_sims = []

  # Generate the LCS table of the interaction models
  for i in range(len_ptn+1):
    for j in range(len_input+1):
      flag, env_sim = ESMIC(pattern[i-1], input[j-1], pattern_prev_msg, input_prev_msg, pattern_env, input_env, d_threshold, 5, 0.8)
      env_sims.append(env_sim)
      if flag:  # Env State & Message Identity Checking (ESMIC) function usage example.
          LCS[i][j] = LCS[i-1][j-1] + 1
          if not pattern_prev_msg:                                                                                        # You need to check the previously matched messages for checking the message delivery intervals of the two messages, respectively
            pattern_prev_msg = pattern[i-1]
            input_prev_msg = input[j-1]
      else:
        LCS[i][j] = max(LCS[i][j-1], LCS[i-1][j])

  # Extract the LCS from the table
  ret = []
  if LCS[len_ptn][len_input] == 0: return None
  else:
    current = 0
    for i in range(1,len_ptn+1):
      for j in range(1, len_input+1):
        if LCS[i][j] > current:
          current += 1
          if pattern[i-1].time >= 25.0: ret.append(pattern[i-1])  # TODO should be modified by model
    return ret, sum(env_sims)/len(env_sims)

def GetPattern(im_pattern, im_input, d_threshold): # LCS pattern extraction function
  if im_pattern == None:
    return im_input
  generated_lcs = []
  generated_avg_env_sim = []
  T = [0, 25, 45, 65, 85]
  for t in T:
    generated_pattern, avg_env_sim = PatternExtractor(InteractionSeqSlicer(im_pattern, t), InteractionSeqSlicer(im_input, t), d_threshold)
    generated_lcs.append(generated_pattern)
    generated_avg_env_sim.append(avg_env_sim)
  return GetMaxContentLCS(generated_lcs, generated_avg_env_sim)

def GetMaxContentLCS(generated_lcs, generated_avg_env_sim): # GetMaxContentLCS among the generated LCSs
  ret_max = None
  ret_max_env = None
  max_contents = -1
  max_len = -1
  for idx, lcs_pattern in enumerate(generated_lcs):
    contents = set()
    for message in lcs_pattern:
      contents.add(message[1])
    if len(contents) > max_contents: # Compare the number of types of contents in LCS pattern
      max_contents = len(contents)
      ret_max = lcs_pattern
      ret_max_env = generated_avg_env_sim[idx]
      max_len = len(lcs_pattern)
    elif len(contents) == max_contents: # If the number of content-types are same, select the shorter one.
      if max_len > len(lcs_pattern):
        max_contents = len(contents)
        ret_max = lcs_pattern
        ret_max_env = generated_avg_env_sim[idx]
        max_len = len(lcs_pattern)
  return ret_max, ret_max_env


def InteractionSeqSlicer(im, time): # Only slice the message sequences by the time TODO: If necessary, cover Env slicing too?
  ret = copy.deepcopy(im)
  ret_interaction = []
  for message in im[2]:
    if float(message[0]) >= time:
      ret_interaction.append(copy.deepcopy(message))
  ret[2] = copy.deepcopy(ret_interaction)
  return ret

"""### SPADE Pattern Mining

"""

def SequenceDBBuilder(IMs):
  # IMs(interaction models) - 0: seqid, 1: P/F, 2: messages, 3: environmental states
  seqDB = set() # set of events (seqid, time(eventid), command-sender-receiver-delay)

  for m in IMs:
    for i in range(len(m[2])):
      msg_t = float(m[2][i][0]) # obtain message time
      if msg_t < 25.00: # only consider the messages after 25.00 sec
        continue
      msg = m[2][i];
      # delay categorization
      prev_msg = m[2][i-1] if (i-1)>=0 else m[2][0]
      delay = int(msg_t - float(prev_msg[0]))
      # message is categorized using command, sender role, receiver role, and delay
      event = (m[0], msg_t, (msg[1]+"-"+msg[3]+"-"+msg[5]+"-"+str(delay)))
      seqDB.add(event)
  return seqDB

def VerticalIdListBuilder(seqDB):
  vIdList = {} # key: message type (element), value: list of (seqid, time)
  for e in seqDB:
    ids = e[0:2]
    if e[2] not in vIdList:
      vIdList[e[2]]=[]
    vIdList[e[2]].append(ids)
  return vIdList

def FrequentElementExtractor(vIdList, sup_threshold): # filter out sequence atoms based on the support threshold
  frequentEleSet = {} # key: sequence atom, value: list of (seqid, time)
  for key, value in vIdList.items():
    s=set()
    for v in value: # count how many sequences has the sequence atom
      s.add(v[0])
    if len(s) >= sup_threshold:
      frequentEleSet[key] = value
  return frequentEleSet

def TemporalJoin(element_a, element_b): # temporal join two sequence atoms' id lists
  resultList = {} # key: new sequence atom (combining element_a & element_b), value: list of (seqid, time)

  for event_a in element_a[1]:
    for event_b in element_b[1]:
      if event_a[0] == event_b[0]: # seqid
        if event_a[1] < event_b[1]: # time
          newSeq = tuple(element_a[0]+[element_b[0][-1]])
          if newSeq not in resultList: 
            resultList[newSeq] = set()
          resultList[newSeq].add(event_b)
        elif event_a[1] > event_b[1]: # time
          newSeq = tuple(element_b[0]+[element_a[0][-1]])
          if newSeq not in resultList: 
            resultList[newSeq] = set()
          resultList[newSeq].add(event_a)
        elif element_a[0][-1] != element_b[0][-1]:
          newSeq_1 = tuple(element_a[0][:-1] + sorted([element_a[0][-1],element_b[0][-1]]))
          if newSeq_1 not in resultList: 
            resultList[newSeq_1] = set()
          resultList[newSeq_1].add(event_b)
          newSeq_2 = tuple(element_b[0][:-1] + sorted([element_a[0][-1],element_b[0][-1]]))
          if newSeq_1 != newSeq_2: 
            if newSeq_2 not in resultList: 
              resultList[newSeq_2] = set()
            resultList[newSeq_2].add(event_b)
  return resultList

def Frequent2ElementSequenceExtractor(freqEleList, sup_threshold): # extract all the frequent 2-sequences
  # build a horizontal database from vertical id list of elements
  horizontalDB = {} # key: seqid, value: list of (element, time)
  for e, idlist in freqEleList.items():
    for ids in idlist:
      if ids[0] not in horizontalDB:
        horizontalDB[ids[0]] = []
      horizontalDB[ids[0]].append([e, ids[1]])

  # construct all the 2-sequence in the database, and count the number of 2-sequence
  count2EleSeq = {} # key: 2-sequence, value: number of the 2-sequence
  for sid, sequence in horizontalDB.items():
    for index_a, event_a in enumerate(sequence):
      for index_b, event_b in enumerate(sequence[index_a+1:]):
        # compare the time of two events, and construct 2-sequence
        if event_a[1] <= event_b[1]:
          twoEleSeq = (event_a[0], event_b[0])
        else: 
          twoEleSeq = (event_b[0], event_a[0])
        # count the number of 2-sequence
        if twoEleSeq not in count2EleSeq:
          count2EleSeq[twoEleSeq] = 0
        count2EleSeq[twoEleSeq] += 1

  # based on elements of 2-sequence, temporal join two id lists to get the 2-sequence's id lists
  newSeqEleIdList = {} # key: 2-sequence, value: set of (seqid, time)
  for twoSeq, count in count2EleSeq.items():
    if count < sup_threshold:
      continue
    if twoSeq[0] == twoSeq[1]: 
      continue
    joinedIdList = TemporalJoin([[twoSeq[0]], freqEleList[twoSeq[0]]], [[twoSeq[1]], freqEleList[twoSeq[1]]])
    for seq, idlist in joinedIdList.items():
      if seq not in newSeqEleIdList:
        newSeqEleIdList[seq] = set()
      newSeqEleIdList[seq].update(idlist)
  
  return FrequentElementExtractor(newSeqEleIdList, sup_threshold)

def EnumerateFrequentSequence(freqEleList, freqSeqEle, sup_threshold): # find the remaining sequence atoms (3-sequences, 4-sequences, ...)
  # temporal join each two sequence atoms to construct new sequence atoms
  newSeqEleIdList = {}
  for index_a, seq_a in enumerate(freqEleList.keys()):
    for index_b, seq_b in enumerate(list(freqEleList.keys())[index_a+1:]):
      joinedIdList = TemporalJoin([list(seq_a), freqEleList[seq_a]], [list(seq_b), freqEleList[seq_b]])
      for seq, idlist in joinedIdList.items():
        if seq not in newSeqEleIdList:
          newSeqEleIdList[seq] = set()
        newSeqEleIdList[seq].update(idlist)
  newSeqEleIdList = FrequentElementExtractor(newSeqEleIdList, sup_threshold)

  # store new frequent sequences in freqSeqEle
  for s, idlist in newSeqEleIdList.items():
    if s not in freqSeqEle:
      freqSeqEle[s] = set()
    freqSeqEle[s].update(idlist)
    
  # Breadth-first-search to contruct new sequence atoms
  if newSeqEleIdList:
    EnumerateFrequentSequence(newSeqEleIdList, freqSeqEle, sup_threshold)

  return freqSeqEle

def SPADE_PatternMining(IMs, sup_threshold):
  print("START Sequence DB BUILDER\n")
  sequences = SequenceDBBuilder(IMs)
  print("START VerticalIDListBuilder\n")
  singleIdList = VerticalIdListBuilder(sequences)
  print("START FrequentElementExtractor\n")
  frequentMsgList = FrequentElementExtractor(singleIdList, sup_threshold)
  print("START FrequentElement2SequenceExtractor\n")
  frequent2MsgSeqList = Frequent2ElementSequenceExtractor(frequentMsgList, sup_threshold)
  freqSeqEle = {} # key: sequence atoms (3-sequence, 4-sequence,...), value: set of (seqid, time)
  print("START EnumFrequentSequence\n")
  frequentMsgSeqList = EnumerateFrequentSequence(frequent2MsgSeqList, freqSeqEle, sup_threshold)
  # frequentMsgSeqList.update(frequent2MsgSeqList)
  # frequentMsgSeqList.update(frequentMsgList)

  # Frequency-based sorting
  frequency = {}
  for seq, idlist in frequentMsgSeqList.items():
    s=set()
    for id in idlist:
      s.add(id[0])
    frequency[seq] = len(s)
  sortedFrequency = sorted(frequency.items(), key=lambda x: x[1], reverse=True)
  sortedFrequentSeqList = []
  for f in sortedFrequency:
    sortedFrequentSeqList.append((f[0], f[1], frequentMsgSeqList[f[0]]))

  return sortedFrequentSeqList

def SPADEListtoString(item_list):
  ret = ''
  for item_ in item_list:
    ret += item_ + "/"
  return ret[:-1]

def SPADESettoString(item_set):
  ret = ''
  for item_tuple in item_set:
    for item_ in item_tuple:
      ret += str(item_) + ":"
    ret = ret [:-1]
    ret += "/"
  return ret[:-1]

def RunSPADE(IM):
  s_threshold = len(IM)*0.9
  frequentSequenceSet = SPADE_PatternMining(IM, s_threshold)
  print(frequentSequenceSet)
  print(len(frequentSequenceSet))

  f = open(join(V_PATH, 'SPADE_0.txt'), 'w')
  for item in frequentSequenceSet:
    f.write(SPADEListtoString(item[0]) + '\n' + SPADESettoString(list(item[2])) + '\n')
  f.close()

"""### LOGLINER & LOGFAULTFLAGGER"""

import math

def LogAbstractor(IMs):
  # IMs(interaction models) - 0: seqid, 1: P/F, 2: messages, 3: environmental states
  logs = [] # list of logs [logid, [line1, line2, line3, ...]]   line: command-sender-receiver

  for m in IMs:
    lines = set()
    if m[1] == "FALSE":
      for i in range(len(m[2])):
        msg_t = float(m[2][i][0]) # obtain message time
        if msg_t < 25.00: # only consider the messages after 25.00 sec
          continue
        msg = m[2][i];
        # log is abstracted to command-sender_role-receiver_role
        line = msg[1]+"-"+msg[3]+"-"+msg[5]
        lines.add(line)
      logs.append([m[0], lines])
  return logs

def LogLineIDFBuilder(logs, corpus): # build a corpus that contains unique log lines and calculate the idf for each logline
  logLineCount = {} # key: logline, value: number of logs that contain the logline
  # count the frequency of each log line in the whole log files
  for log in logs:
    for line in log[1]:
      if line not in corpus:
        corpus.append(line)
        logLineCount[line] = 0
      logLineCount[line]+=1
  # calculate line-IDF
  numberOfLogs = len(logs)
  lineIDF = {} # key: logline, value: line-IDF
  for key, value in logLineCount.items():
    lineIDF[key] = math.log(numberOfLogs / value)
  return lineIDF

def LogLineFlagger(logs, lineIDF, N): # find the N rarest log lines in each log file
  logLineIDF = [] # list of logs with their flagged log lines [logid, [(line1, idf), ..., (lineN, idf)]]

  for log in logs:
    IDFList = []
    for line in log[1]:
      IDFList.append((line, lineIDF[line]))
    # sort loglines by line-idf
    IDFList.sort(key = lambda x: x[1], reverse=True)
    if len(IDFList) <= N:
      logLineIDF.append([log[0], IDFList])
    else:
      logLineIDF.append([log[0], IDFList[0:N]])
  return logLineIDF

def Vectorization(logLineIDF, corpus):
  logVector = [] # list of logs with their vector [logid, vector], vector dimension is length of corpus (number of unique log lines)
  for log in logLineIDF:
    v = [0] * len(corpus)
    for line in log[1]:
      v[corpus.index(line[0])] = line[1]
    logVector.append([log[0], v])
  return logVector

def LogLiner(IMs, N):
  corpus = []
  logs = LogAbstractor(IMs)
  lineIDF = LogLineIDFBuilder(logs, corpus)
  flaggedLines = LogLineFlagger(logs, lineIDF, N)
  logVecotrs = Vectorization(flaggedLines, corpus)
  return flaggedLines, logVecotrs

def RunLogLiner(IM):
  flaggedLines, logVecotrs = LogLiner(IM, 5)
  print(flaggedLines)

"""####Processing ideal examples of fault class"""
def FaultClassProcess():
  class_PATH = join(V_PATH, 'Ideal/')
  classLogs = []
  for filename in os.listdir(class_PATH):
    strings = filename.split('_')
    if 'plnData_class' in filename:
      classFile = open(join(class_PATH,filename), 'r')
      lines = classFile.readlines()
      split_count = -1
      veh_roles = {}
      iteraction = []
      for i in range(4, len(lines)):
        message = []
        line = lines[i]
        line = ' '.join(line.split()) # Preprocessing
        line = line.replace(' -', '')
        line_items = line.split(' ')
        if 'MF_Leave' in line: # FollowerLeave type checking for veh role analysis
          split_count = 2
        elif 'EF_Leave' in line:
          split_count = 1
        if len(line_items) > 5: # For each message items => [time, command_sent, sender, sender_role, receiver, receiver_role]
          time = line_items[0]
          message.append(time)
          command_sent = line_items[4]
          message.append(command_sent)
          if command_sent == 'MERGE_REQ': # Role managmenet code by each role changing CS-level operations
            veh_roles[line_items[1]] = 'Leader'
            if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
              veh_roles[line_items[5]] = 'Leader'
          elif command_sent == 'SPLIT_REQ':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles[line_items[1]] = 'Leader'
          elif command_sent == 'LEAVE_REQ':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles[line_items[1]] = 'Left'
            if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
              veh_roles[line_items[5]] = 'Leader'
          elif command_sent == 'VOTE_LEADER':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles[line_items[1]] = 'Left'
          elif command_sent == 'MERGE_DONE':
            if line_items[1] not in veh_roles.keys() or veh_roles[line_items[1]] != 'Left':
              veh_roles.pop(line_items[1])
          elif command_sent == 'SPLIT_DONE':
            split_count -= 1
            if split_count != 0: # Just SPLIT operation && the first SPLIT in MF-LEAVE && LEADER-LEAVE
              if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
                veh_roles[line_items[5]] = 'Leader'
            elif split_count == 0: # The last SPLIT in MF-LEAVE / EF-LEAVE
              if line_items[5] not in veh_roles.keys() or veh_roles[line_items[5]] != 'Left':
                veh_roles[line_items[5]] = 'Left'
          sender = line_items[1]
          message.append(sender)
          if line_items[1] not in veh_roles.keys():
            message.append('Follower')
          else:
            message.append(veh_roles[line_items[1]])
          receiver = line_items[5]
          message.append(receiver)
          if line_items[5] not in veh_roles.keys():
            message.append('Follower')
          else:
            message.append(veh_roles[line_items[5]])
          # interaction.append(message) // TODO
      classFile.close()
      # classLogs.append([int(strings[0]), 'FALSE', interaction]) // TODO

def LogFaultFlagger(IMs, faultClassLogs, N):
  corpus = []
  logs = LogAbstractor(IMs)
  processedClassLogs = LogAbstractor(faultClassLogs)

  lineIDF = LogLineIDFBuilder(logs, corpus)
  # calculate fault count
  faultCount = {}
  for log in processedClassLogs:
    for line in log[1]:
      if line not in faultCount:
        faultCount[line] = 0
      faultCount[line]+=1
  # update IDF with fault count
  for key in faultCount.keys():
    lineIDF[key]*=(1+faultCount[key])

  flaggedLines = LogLineFlagger(logs, lineIDF, N)
  logVecotrs = Vectorization(flaggedLines, corpus)
  return flaggedLines, logVecotrs

def RunLogFaultFlagger(IM, classLogs):
  flaggedLines, logVecotrs = LogFaultFlagger(IM, classLogs, 5)
  print(flaggedLines)

"""## Fuzzy Clustering"""

import random
import math

def FCM(cl_type, IM_): # TODO cl_type: FCM, PFS (Picture Fuzzy Set), KS2M
  INIT_SIM_THRESHOLD = 0.6
  MAX_INIT_SIM_THRESHOLD = 0.8
  C_VALUE = 6
  SENSITIVITY_THRESHOLD = 0.3
  DELAY_THRESHOLD = 0.5
  SIM_THRESHOLD = 0.8
  MAX_ITERATION = 10000
  m = 2 # Fuzzy value

  simvalues = [[random.randrange(0,1) for i in range(len(IM_))] for j in range(C_VALUE)]
  memberships = [[random.randrange(0,1) for i in range(len(IM_))] for j in range(C_VALUE)]
  diss = [[random.randrange(0,1) for i in range(len(IM_))] for j in range(len(C_VALUE))]
  clusters = []
  iterations = 0

  while True: # Initial selection of C models from the whole models
    initial_sims = []
    patterns = []
    for i in range(0, C_VALUE): # Select C numbers of models
      item = IM_[random.randint(0,len(IM_))]
      if item not in patterns:
        patterns.append(item)
    for i in range(0, len(patterns)):
      for j in range(i, len(patterns)):
        sim_value = CAFCASimCal(patterns[i], patterns[j], DELAY_THRESHOLD) # Calculate the LCS_Sim values for each combination of initally selected models
        if sim_value > MAX_INIT_SIM_THRESHOLD: # If two of them are highly simialr, choose the other models
          continue
        initial_sims.append(sim_value)
    if len(initial_sims) > 0 and sum(initial_sims)/len(initial_sims) < INIT_SIM_THRESHOLD: # If the average of the LCS_sim values of the models is non-acceptable, find other set of models
      break

  prev_objs = -1 # Sum of Squared Errors for Fuzzy C-means clustering

  while iterations < MAX_ITERATION:
    # Similarity & Dissimilarity value calculation between patterns and models
    for j in range(C_VALUE):
      for k in range(len(IM_)):
        simvalues[j][k] = CAFCASimCal(patterns[j], IM_[k], DELAY_THRESHOLD)
        diss[j][k] = 1 - simvalues[j][i]

    # Membership caluclation & clustering
    clusters = []
    for j in range(C_VALUE):
      cluster = []
      temp = 0
      for k in range(len(IM_)):
        for i in range(C_VALUE):
          temp += math.pow(diss[j][k] / diss[i][k], 2/m-1)
        memberships[j][k] = 1 / temp
        if memberships[j][k] > SIM_THRESHOLD: # Clustering
          cluster.append(copy.deepcopy(IM_[k]))
      clusters.append(copy.deepcopy(cluster))

    # Pattern update
    patterns.clear()
    for j in range(C_VALUE):
      pattern = None
      for i in range(len(clusters[j])):
        pattern, env_sim = GetPattern(pattern, clusters[j][i])
      patterns.append(copy.deepcopy(pattern))

    # Objective value calculation -> Basic FCM version
    objs = 0
    for j in range(C_VALUE):
      for k in range(len(IM_)):
        objs += math.pow(memberships[j][k], 2) * math.pow(diss[j][k],2)

    # for p in range(C_VALUE):  # Initial version based on Fast Fuzzy Algorithm
    #   val = 0
    #   denom = 0
    #   for i,j in range(len(IM_)):
    #     val += math.pow(memberships[p][i], FUZZY_THRESHOLD) * math.pow(memberships[p][j], FUZZY_THRESHOLD) * diss[i][j]
    #     denom += math.pow(memberships[p][j],FUZZY_THRESHOLD)
    #   objs += val / 2 * denom

    if prev_objs != -1 and math.abs(objs - prev_objs) < SENSITIVITY_THRESHOLD:
      break
    else:
      prev_objs = objs
    iterations += 1

"""# **Extended SBFL for Pattern Elaboration**

Extend SBFL approach to make the patterns more elaborative using failed & passed logs

## SBFL Executor
"""

import math
def CheckPatternAppearance_ESMIC(pattern, interaction_model, pattern_prev, interaction_model_prev, pattern_env, interaction_model_env, d_threshold):
  result = False
  if len(pattern)==0: #Initial point of recursive call
    return True
  else:
    head = pattern[0]
    tail = pattern[1:]
    for i in range(len(interaction_model)):
      if ESMIC(head, interaction_model[i], pattern_prev, interaction_model_prev, pattern_env, interaction_model_env, d_threshold, 5, 0.8): #Comparison of head message of pattern and current message in interaction model
        result = CheckPatternAppearance(tail, interaction_model[i+1:], head, interaction_model[i], pattern_env, interaction_model_env, d_threshold, 5, 0.8) #If exist, check rest of the pattern and rest of the interaction model
        break
    return result

def CheckPatternAppearance(pattern, interaction_model): # If for LCS: len(LCS(Pattern, interaction_model)) / len(Pattern)
  result = False

  if len(pattern)==0: #Initial point of recursive call
    return True
  else:
    head = pattern[0]
    tail = pattern[1:]
    for i in range(len(interaction_model)):
      if MCT(head,interaction_model[i]): #Comparison of head message of pattern and current message in interaction model
        result = CheckPatternAppearance(tail, interaction_model[i+1:]) #If exist, check rest of the pattern and rest of the interaction model
        break
    return result

def CheckNumberofPassFail(pattern, IM):
  numpass = 0
  numfail = 0
  for i in IM: #For all im in IM, check appearance and count pass and fail cases when pattern appears in im.
    if CheckPatternAppearance(pattern,i[2]):
      if i[1]=="FALSE":
        numfail += 1
      else:
        numpass += 1
  return (numpass,numfail)

def SBFLScore(technique,numpass,numfail,IM):
  totalpassed=0
  totalfailed=0
  for i in IM:
    if i[1] == "FALSE":
      totalfailed += 1
    else:
      totalpassed += 1
  if technique=="Tarantula":
    if numfail== 0 and numpass ==0: return 0
    return (numfail/totalfailed)/((numfail/totalfailed)+(numpass/totalpassed))
  elif technique == "Ochiai":
    if numfail== 0 and numpass ==0: return 0
    return numfail/math.sqrt(totalfailed*(numfail+numpass))
  elif technique == "Op2":
    return numfail - (numpass/(totalpassed+1))
  elif technique == "DStar":
    if numpass+(totalfailed-numfail)== 0: return math.inf
    return math.pow(numfail,x)/(numpass+(totalfailed-numfail))

def SBFL(patterns,IM,n,technique):
  patternscorelist = []
  for i in patterns:
    p,f = CheckNumberofPassFail(i,IM)
    print(p,f)
    score = SBFLScore(technique,p,f,IM)
    patternscorelist.append((i,score))
  patternscorelist.sort(key=lambda x:-x[1])
  return patternscorelist[0:n]

def RunSBFL(IM):
  print(IM[0])
  print(len(IM))
  print(SBFL([[["0.00","SPLIT_REQ","veh","Leader","veh.4","Follower"], ["0.00","SPLIT_REQ","veh1","Leader","veh1.4","Follower"]]],IM,1,"Tarantula"))
  print(SBFL([IM[0][2]],IM,1,"Tarantula"))

"""# **Evaluation**"""

#F1P Evaluator

import math

def sumContributionsIM(elements,elem_clusters): #element: [InteractionModel], elem_clusters: Dict: str - [int]
    ret = 0

    for elem in elements:
        shares_elem = len(elem_clusters[elem[0]])
        ret += 1 / shares_elem

    return ret

def sumContributions(elements,elem_clusters): #element: [str], elem_clusters: Dict: str - [int]
    ret = 0

    for elem in elements:
        shares_elem = len(elem_clusters[elem])
        ret += 1/ shares_elem

    return ret

def matchedElements(cl_cl,cl_or):
    matched = []

    for im in cl_cl:
        for elem in cl_or:
            if im[0] == elem: matched.append(im[0])

    return matched


def EvaluateF1P(oracle,index,cluster): #oracle: [[str]], index: [str], cluster: [[InteractionModel]]
    ret = []
    c_element_index = {} #Dictionary of the set of location of each Interaction model in cluster
    o_element_index = {} #Dictionary of the set of location of each Interaction model in oracle (Ground-truth)

    #Generate Element-wise dictionary of Oracle and the formed cluster
    for id in index:
        c_index  = []
        o_index = []

        for cl in oracle:
            if(id in cl): o_index.append(oracle.index(cl))

        for IMs in cluster:
            for IM in IMs:
                if IM[0] == id: c_index.append(cluster.index(IMs)) #IM[0] : id of the IM

        o_element_index[id] = o_index
        c_element_index[id] = c_index

    bestMatches_cl = []
    bestMatches_or = []
    bestMatch = 0
    cl_cl_cntb = 0
    cl_or_cntb = 0
    matched_cntb = 0
    tempMatch = 0

    #Formed cluster의 결과에서 단일 cluster를 기준으로 oracle에서 가장 Best한 match값을 가지는
    #oracle_cluster 와의 contribution sum을 해당 cluster의 match값으로 설정함. -> F_(C,O)

    for cl_cl in cluster: #cl_cl: [InteractionModel]
        for cl_or in oracle: #cl_or: [str]
            cl_cl_cntb = sumContributionsIM(cl_cl,c_element_index)
            cl_or_cntb = sumContributions(cl_or, o_element_index)
            matched = matchedElements(cl_cl,cl_or)

            matched_cntb = sumContributions(matched,c_element_index)

            tempMatch = math.pow(matched_cntb,2) / (cl_cl_cntb*cl_or_cntb)
            if tempMatch > bestMatch:
                bestMatch = tempMatch
        bestMatches_cl.append(math.sqrt(bestMatch))
        bestMatch = 0

    F_C_O = 0
    for mat in bestMatches_cl:
        F_C_O += mat/len(bestMatches_cl)

    #F_C_O *= (1/0.7) #What is this?

    ret.append(F_C_O)

    for cl_or in oracle:
        for cl_cl in cluster:
            cl_cl_cntb = sumContributionsIM(cl_cl,c_element_index)
            cl_or_cntb = sumContributions(cl_or,o_element_index)
            matched = matchedElements(cl_cl,cl_or)

            matched_cntb = sumContributions(matched,o_element_index)

            tempMatch = math.pow(matched_cntb,2) / (cl_cl_cntb*cl_or_cntb)
            if tempMatch > bestMatch:
                bestMatch = tempMatch
        bestMatches_or.append(math.sqrt(bestMatch))
        bestMatch = 0

    F_O_C = 0
    for mat in bestMatches_or:
        F_O_C += mat/len(bestMatches_or)

    #F_O_C *= (1/0.7)

    ret.append(F_O_C)
    ret.append(2*F_C_O*F_O_C/(F_C_O+F_O_C))

    return ret


def main():
  IM, FIM = IMGenerator()
  IMtoTxt(IM,'InteractionModels.txt')
  IMtoTxt(FIM, 'FailedInteractionModels.txt')
  # FIM = TxttoIM('FailedInteractionModels.txt')
  RunSPADE(FIM)

if __name__ == "__main__":
  main()

# """# **Git synchronizing**
#
# If you finish your work, please run the following code to update the git repository.
#
# """
#
# # Commented out IPython magic to ensure Python compatibility.
# PROJECT_PATH = join(gdrive_root, 'CAFCA---Context-Aware-Fuzzy-Clustering-for-Analyzing-Interaction-Failures')
# GIT_USERNAME = 'abalon1210'
# GIT_TOKEN = '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' # SW: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 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 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 HS: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 LJ: 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
# GIT_REPO = 'CAFCA---Context-Aware-Fuzzy-Clustering-for-Analyzing-Interaction-Failures'
#
# #print(PROJECT_PATH)
#
# GIT_PATH = 'https://' + GIT_TOKEN + '@github.com/' + GIT_USERNAME + '/' + GIT_REPO + '.git'
# #print("GIT_PATH: ", GIT_PATH)
#
# # %cd '{PROJECT_PATH}'
# !git config --global user.email 'rienshaliu@gmail.com'           # Your Github e-mail  # SW: abalon1210@daum.net LJ: rienshaliu@gmail.com HS: hansu1125@gmail.com
# !git config --global user.name 'Rienshaliu'                     # Your Github ID      # SW: abalon1210          LJ: Rienshaliu              HS: hsKim25
#
# # !git reset
# !git add 'CAFCA-Colab.ipynb'
# # !git commit -m 'Implement the LOGFAULTFLAGGER'                           # Add your commit message here
# !git remote rm origin
# !git remote add origin "{GIT_PATH}"
# #!git remote -v
# #!git push origin :BRANCH_NAME   #Delete the branch in remote
# !git push origin master